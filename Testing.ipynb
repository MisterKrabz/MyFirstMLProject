{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe112704-296d-42ef-9b04-2cd8232157a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class article: \n",
    "    def __init__(self, title, sent):\n",
    "        self.title = title\n",
    "        self.sent = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e668c86c-a4a3-4b39-a8f1-ddacc7dd7984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Successfully read 4846 lines from ./data/all-data.csv\n",
      "4846\n",
      "Successfully read 2264 lines from ./data/sentences_AllAgree.txt\n",
      "7054\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./data/all-data.csv\"\n",
    "\n",
    "sentences = []\n",
    "sentiments = []\n",
    "data_lines = []\n",
    "\n",
    "# stores article objects \n",
    "articles = []\n",
    "print(len(articles))\n",
    "\n",
    "try:\n",
    "    with open(file_path, encoding='latin1') as f:\n",
    "        # f.readlines() reads the entire file into a list of strings, one for each line\n",
    "        data_lines = f.readlines()\n",
    "        print(f\"Successfully read {len(data_lines)} lines from {file_path}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file '{file_path}' was not found. Please check the file path.\")\n",
    "\n",
    "for line in data_lines:\n",
    "    # Split each line at the first comma\n",
    "    parts = line.strip().split(',', 1)\n",
    "    \n",
    "    # The rest of your logic was correct!\n",
    "    if len(parts) == 2:\n",
    "        sentiment = parts[0]\n",
    "        sentence = parts[1].strip(' \"') # Removes quotes and spaces from the ends\n",
    "        \n",
    "        sentiments.append(sentiment)\n",
    "        sentences.append(sentence)\n",
    "\n",
    "        articles.append(article(sentence, sentiment))\n",
    "        \n",
    "\n",
    "print(len(articles))\n",
    "\n",
    "# MORE DATA!!!\n",
    "file_path1 = \"./data/sentences_AllAgree.txt\"\n",
    "\n",
    "data_lines1 = []\n",
    "\n",
    "try:\n",
    "    with open(file_path1, encoding='utf-8') as f:\n",
    "        data_lines1 = f.readlines()\n",
    "        print(f\"Successfully read {len(data_lines1)} lines from {file_path1}\")\n",
    "except IOError as e:\n",
    "    print(f\"ERROR: An error occurred while writing to the file '{file_path1}': {e}\")\n",
    "\n",
    "for line in data_lines1: \n",
    "    parts = line.strip().split('.@', 1)\n",
    "\n",
    "    if len(parts) == 2:\n",
    "        sentiment = parts[1]\n",
    "        sentence = parts[0]\n",
    "        \n",
    "        sentiments.append(sentiment)\n",
    "        sentences.append(sentence)\n",
    "\n",
    "        articles.append(article(sentence, sentiment))\n",
    "        \n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f1ff2383-ac4d-4208-9213-53347c23f088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5643\n",
      "1411\n",
      "<__main__.article object at 0x3010494d0>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "training, test = train_test_split(articles, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(len(training))\n",
    "print(len(test))\n",
    "\n",
    "print(training[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd3b0f8c-d62a-4930-aac1-b1ad31858226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x is what we pass in (the title) y is what we get (the sentiment) \n",
    "train_x = [x.title for x in training]\n",
    "train_y = [x.sent for x in training]\n",
    "\n",
    "test_x = [x.title for x in test]\n",
    "test_y = [y.sent for y in test]\n",
    "\n",
    "train_x[0]\n",
    "train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "83f2395a-92a5-49e9-a052-73ba564be2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5643, 9289)\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 14 stored elements and shape (1, 9289)>\n",
      "  Coords\tValues\n",
      "  (0, 5972)\t1\n",
      "  (0, 5134)\t1\n",
      "  (0, 1001)\t1\n",
      "  (0, 8456)\t2\n",
      "  (0, 3098)\t2\n",
      "  (0, 5533)\t2\n",
      "  (0, 2079)\t1\n",
      "  (0, 6619)\t1\n",
      "  (0, 5905)\t2\n",
      "  (0, 4305)\t1\n",
      "  (0, 8359)\t1\n",
      "  (0, 7442)\t1\n",
      "  (0, 6746)\t1\n",
      "  (0, 206)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# makes a new instance of the vectorizer \n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# vectorization stage \n",
    "#\n",
    "# this method is composed of two steps: \n",
    "# 1. fit: builds a vocabulary vector from the training data\n",
    "# 2. transform: transforms the training data into a document-term matrix\n",
    "#\n",
    "# iterates through all the titles in the train_x list and builds a complete dictionary of all \n",
    "# the unique words it encounters. Then makes a matrix that looks like this: \n",
    "#\n",
    "# ex:              I     love    hate    cars \n",
    "# I love cars:     1       1      0        1\n",
    "# I hate cars:     1       0      1        0\n",
    "#\n",
    "# Although the finished product is a matrix, each row corresponds to a article title\n",
    "# train_x_vectors is a \"sparce matrix\" (basically a more efficient 2d array where the 0 values are not stored)\n",
    "train_x_vectors = vectorizer.fit_transform(train_x)\n",
    "\n",
    "# we dont want to fit the vectorizer again on the test data, we just want to transform it\n",
    "# so we use the same vocabulary that was built from the training data\n",
    "test_x_vectors = vectorizer.transform(test_x)\n",
    "\n",
    "print(train_x_vectors.shape)  \n",
    "print(train_x_vectors[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ce84e",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0be7858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive']\n",
      "['positive']\n",
      "['positive']\n",
      "['neutral']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# there are many different algorithms that can be used to classify data,\n",
    "# such as decision trees, random forests, logistic regression, etc.\n",
    "# here is using support vector machine (SVM) classifier\n",
    "\n",
    "# fitting the model: the main step\n",
    "#\n",
    "# choosing a new, untrained classifier algorithm and train it to find patterns \n",
    "# between numerical data and labels \n",
    "#\n",
    "# SVC = support vector classifier machine learning algorithm \n",
    "# clf_svm is the classifier object that can be used to make predictions later \n",
    "# kernel is a parameter that defines the type of decision boundary to be used. \n",
    "# this only pretains to SVM classifiers \n",
    "#\n",
    "# kernel = 'linear' means that the algorithm will try to find a linear decision \n",
    "# boundary (where the data is seperated into 2 classes divided by a straight line\n",
    "# and to predict, it will use the linear decision boundary to classify new data). \n",
    "# for non linear (rbf, poly, etc.) the algorithm will try to find a non-linear \n",
    "# decision boundary\n",
    "clf_svm = SVC(kernel='linear')\n",
    "\n",
    "# fitting this classifier to the training data \n",
    "clf_svm.fit(train_x_vectors, train_y)\n",
    "\n",
    "test_x[0]\n",
    "test_x_vectors[0]\n",
    "print(clf_svm.predict(test_x_vectors[0]))\n",
    "print(clf_svm.predict(test_x_vectors[56]))\n",
    "print(clf_svm.predict(test_x_vectors[100]))\n",
    "print(clf_svm.predict(test_x_vectors[500]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb1d59d",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "71df00ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive'], dtype='<U8')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dec = DecisionTreeClassifier()\n",
    "clf_dec.fit(train_x_vectors, train_y)\n",
    "\n",
    "clf_dec.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2643fa37",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dc08cb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive'], dtype='<U8')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = DecisionTreeClassifier()\n",
    "clf_gnb.fit(train_x_vectors, train_y)\n",
    "\n",
    "clf_gnb.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5dcdd2",
   "metadata": {},
   "source": [
    "Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3e24f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive'], dtype='<U8')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression(max_iter=1000)\n",
    "clf_log.fit(train_x_vectors, train_y)\n",
    "clf_log.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "821690ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8462083628632175\n",
      "0.8454996456413891\n",
      "0.8447909284195606\n",
      "0.9930887825624668\n",
      "[0.77690289 0.88824214 0.78947368]\n",
      "[0.7688172  0.88939567 0.78395062]\n",
      "[0.7585266  0.89164786 0.78233438]\n",
      "[0.79893475 0.90691034 0.825     ]\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation \n",
    "\n",
    "print(clf_svm.score(test_x_vectors, test_y))\n",
    "print(clf_dec.score(test_x_vectors, test_y))\n",
    "print(clf_gnb.score(test_x_vectors, test_y))\n",
    "print(clf_log.score(train_x_vectors, train_y))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(test_y, clf_svm.predict(test_x_vectors), average = None, labels = ['positive', 'neutral', 'negative']))\n",
    "print(f1_score(test_y, clf_dec.predict(test_x_vectors), average = None, labels = ['positive', 'neutral', 'negative']))\n",
    "print(f1_score(test_y, clf_gnb.predict(test_x_vectors), average = None, labels = ['positive', 'neutral', 'negative']))\n",
    "print(f1_score(test_y, clf_log.predict(test_x_vectors), average = None, labels = ['positive', 'neutral', 'negative']))\n",
    "\n",
    "# from the output it looks as though all the classifiers perform equally as bad \n",
    "# so this might be a data issue rather than a model issue "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock_predictor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
